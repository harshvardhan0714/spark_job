{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import requests\n",
    "from spark_common import initiate_spark,terminate_spark, get_circle_data , get_process_sensor, get_data_from_api,reading_and_validate_data,train_test_split\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType, IntegerType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "import joblib\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminate_spark(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>LocalSparkSession</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x26862b2d810>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = initiate_spark()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23349\n",
      "20061\n",
      "7354\n",
      "21856\n",
      "9748\n",
      "16430\n",
      "18322\n",
      "17211\n",
      "18589\n",
      "20236\n",
      "14214\n"
     ]
    }
   ],
   "source": [
    "\n",
    "circle_list = get_circle_data()\n",
    "# data = get_process_sensor(circle_list[0]['id'])\n",
    "data= []\n",
    "for id in range(len(circle_list)):\n",
    "    data_complete  = get_process_sensor(circle_list[id]['id'])\n",
    "    data.extend(data_complete )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_file_path = \"schema.yaml\"\n",
    "# Validate the data against the schema\n",
    "validated_df = reading_and_validate_data(spark, data, schema_file_path)\n",
    "\n",
    "# Check if validation was successful\n",
    "if validated_df is not None:\n",
    "    df = validated_df\n",
    "    # df = validated_df.select(\"creation_time\",\"sensor_id\",\"consumed_unit\")\n",
    "    # print(df.count())\n",
    "\n",
    "else:\n",
    "    print(\"Data validation failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187370"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n",
      "83\n"
     ]
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCol=\"sensor_id\", outputCol=\"encoded_sensor_id\")\n",
    "encoded_df = indexer.fit(df).transform(df)\n",
    "numbers_to_partitions = encoded_df.select(\"encoded_sensor_id\").distinct().count()\n",
    "print(encoded_df.select(\"sensor_id\").distinct().count())\n",
    "print(numbers_to_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of partion before: 8\n",
      "number of partion after: 83\n"
     ]
    }
   ],
   "source": [
    "df = encoded_df.drop(\"sensor_id\")\n",
    "num = df.rdd.getNumPartitions()\n",
    "print(\"number of partion before:\",num)\n",
    "repartitoned_df = df.repartition(numbers_to_partitions,['sensor_id'])\n",
    "num1 = repartitoned_df.rdd.getNumPartitions()\n",
    "print(\"number of partion after:\",num1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(key,pdf):\n",
    "    total_rows = len(pdf)\n",
    "    train_rows = int(0.8 * total_rows)\n",
    "    pdf = pdf.sort_values(by=\"creation_time\")\n",
    "    train_df = pdf.iloc[:train_rows]\n",
    "    FEATURES = ['is_holiday','temperature_2m', 'relative_humidity_2m', 'apparent_temperature','precipitation', 'wind_speed_10m','wind_speed_100m', 'lag1', 'lag2','lag3', 'lag4', 'lag5', 'day', 'hour', 'month', 'dayofweek', 'quarter','dayofyear', \"encoded_sensor_id\",'weekofyear', 'year']\n",
    "    TARGET = ['consumed_unit']\n",
    "    \n",
    "    X_train = train_df[FEATURES]\n",
    "    y_train = train_df[TARGET]\n",
    "\n",
    "    # from xgboost import XGBRegressor\n",
    "    xgb_model = XGBRegressor()\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    param_grid = {\n",
    "    'n_estimators': [50, 100, 150, 200, 500],\n",
    "    'max_depth': [5, 10, 15, 20, 30],\n",
    "    'learning_rate': [0.01, 0.1, 0.3, 0.5, 0.7],\n",
    "    'reg_alpha':  [0.001, 0.01, 0.1, 0.5, 1.0]\n",
    "    }\n",
    "\n",
    "    random_search = RandomizedSearchCV(xgb_model,\n",
    "                                        param_distributions=param_grid,\n",
    "                                        n_iter=10,\n",
    "                                        scoring='neg_mean_squared_error',\n",
    "                                        cv=5,\n",
    "                                        # verbose=1,\n",
    "                                        n_jobs=-1,\n",
    "                                        random_state=45)\n",
    "\n",
    "    # Fit the RandomizedSearchCV to the data\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best parameters\n",
    "    best_params = random_search.best_params_\n",
    "    # print(f\"Best Parameters for sensor {i}: {best_params}\")\n",
    "\n",
    "    # Train the model with the best parameters\n",
    "    best_xgb_model = XGBRegressor(n_estimators=best_params['n_estimators'],\n",
    "                                max_depth=best_params['max_depth'],\n",
    "                                learning_rate=best_params['learning_rate'],\n",
    "                                reg_alpha=best_params['reg_alpha'],\n",
    "                                reg_lambda=0.01,\n",
    "                                )\n",
    "    best_xgb_model.fit(X_train, y_train)\n",
    "    model_filename = f\"xgb_model_sensor_{key}.joblib\"\n",
    "    joblib.dump(xgb_model, model_filename)\n",
    "    return X_train\n",
    "# result = (store_part.groupBy(\"sensor_id\").apply(train_test_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_schema = StructType([\n",
    "            StructField(\"temperature_2m\", FloatType(), True),\n",
    "            StructField(\"relative_humidity_2m\", IntegerType(), True),\n",
    "            StructField(\"apparent_temperature\", FloatType(), True),\n",
    "            StructField(\"precipitation\", FloatType(), True),\n",
    "            StructField(\"wind_speed_10m\", FloatType(), True),\n",
    "            StructField(\"wind_speed_100m\", FloatType(), True),\n",
    "            StructField(\"is_holiday\", IntegerType(), True),\n",
    "            StructField(\"lag1\", FloatType(), True),\n",
    "            StructField(\"lag2\", FloatType(), True),\n",
    "            StructField(\"lag3\", FloatType(), True),\n",
    "            StructField(\"lag4\", FloatType(), True),\n",
    "            StructField(\"lag5\", FloatType(), True),\n",
    "            StructField(\"day\", IntegerType(), True),\n",
    "            StructField(\"hour\", IntegerType(), True),\n",
    "            StructField(\"month\", IntegerType(), True),\n",
    "            StructField(\"dayofweek\", IntegerType(), True),\n",
    "            StructField(\"quarter\", IntegerType(), True),\n",
    "            StructField(\"dayofyear\", IntegerType(), True),\n",
    "            StructField(\"encoded_sensor_id\", FloatType(), True),\n",
    "            StructField(\"weekofyear\", IntegerType(), True),\n",
    "            StructField(\"year\", IntegerType(), True),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o476.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 42.0 failed 1 times, most recent failure: Lost task 0.0 in stage 42.0 (TID 423) (host.docker.internal executor driver): java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\r\n\tat org.apache.arrow.memory.util.MemoryUtil.directBuffer(MemoryUtil.java:174)\r\n\tat org.apache.arrow.memory.ArrowBuf.getDirectBuffer(ArrowBuf.java:229)\r\n\tat org.apache.arrow.memory.ArrowBuf.nioBuffer(ArrowBuf.java:224)\r\n\tat org.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:133)\r\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:303)\r\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:276)\r\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:147)\r\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:133)\r\n\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:140)\r\n\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\r\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:418)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\r\n\tat org.apache.arrow.memory.util.MemoryUtil.directBuffer(MemoryUtil.java:174)\r\n\tat org.apache.arrow.memory.ArrowBuf.getDirectBuffer(ArrowBuf.java:229)\r\n\tat org.apache.arrow.memory.ArrowBuf.nioBuffer(ArrowBuf.java:224)\r\n\tat org.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:133)\r\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:303)\r\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:276)\r\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:147)\r\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:133)\r\n\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:140)\r\n\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\r\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m result \u001b[38;5;241m=\u001b[39m repartitoned_df\u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoded_sensor_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mapplyInPandas(split_train_test,schema\u001b[38;5;241m=\u001b[39mresult_schema)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Deployment\\spark_job\\spark\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:899\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    894\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    895\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    896\u001b[0m     )\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 899\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Deployment\\spark_job\\spark\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32md:\\Deployment\\spark_job\\spark\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32md:\\Deployment\\spark_job\\spark\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o476.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 42.0 failed 1 times, most recent failure: Lost task 0.0 in stage 42.0 (TID 423) (host.docker.internal executor driver): java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\r\n\tat org.apache.arrow.memory.util.MemoryUtil.directBuffer(MemoryUtil.java:174)\r\n\tat org.apache.arrow.memory.ArrowBuf.getDirectBuffer(ArrowBuf.java:229)\r\n\tat org.apache.arrow.memory.ArrowBuf.nioBuffer(ArrowBuf.java:224)\r\n\tat org.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:133)\r\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:303)\r\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:276)\r\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:147)\r\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:133)\r\n\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:140)\r\n\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\r\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:418)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\r\n\tat org.apache.arrow.memory.util.MemoryUtil.directBuffer(MemoryUtil.java:174)\r\n\tat org.apache.arrow.memory.ArrowBuf.getDirectBuffer(ArrowBuf.java:229)\r\n\tat org.apache.arrow.memory.ArrowBuf.nioBuffer(ArrowBuf.java:224)\r\n\tat org.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:133)\r\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:303)\r\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:276)\r\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:147)\r\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:133)\r\n\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:140)\r\n\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\r\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n"
     ]
    }
   ],
   "source": [
    "result = repartitoned_df.groupBy(\"encoded_sensor_id\").applyInPandas(split_train_test,schema=result_schema)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temperature_2m</th>\n",
       "      <th>relative_humidity_2m</th>\n",
       "      <th>apparent_temperature</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>wind_speed_10m</th>\n",
       "      <th>wind_speed_100m</th>\n",
       "      <th>creation_time</th>\n",
       "      <th>sensor_id</th>\n",
       "      <th>count</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>...</th>\n",
       "      <th>lag6</th>\n",
       "      <th>lag7</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>month</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>quarter</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30.7</td>\n",
       "      <td>33</td>\n",
       "      <td>31.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.6</td>\n",
       "      <td>2024-03-22 08:00:00</td>\n",
       "      <td>022040a0-beb4-413c-bc3b-e6564fc2699c</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>12</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.7</td>\n",
       "      <td>33</td>\n",
       "      <td>31.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.6</td>\n",
       "      <td>2024-03-22 08:15:00</td>\n",
       "      <td>022040a0-beb4-413c-bc3b-e6564fc2699c</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>12</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30.7</td>\n",
       "      <td>33</td>\n",
       "      <td>31.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.6</td>\n",
       "      <td>2024-03-22 08:30:00</td>\n",
       "      <td>022040a0-beb4-413c-bc3b-e6564fc2699c</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>12</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.7</td>\n",
       "      <td>33</td>\n",
       "      <td>31.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.6</td>\n",
       "      <td>2024-03-22 08:45:00</td>\n",
       "      <td>022040a0-beb4-413c-bc3b-e6564fc2699c</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>12</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31.1</td>\n",
       "      <td>34</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.2</td>\n",
       "      <td>13.8</td>\n",
       "      <td>2024-03-22 09:00:00</td>\n",
       "      <td>022040a0-beb4-413c-bc3b-e6564fc2699c</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>12</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187365</th>\n",
       "      <td>26.1</td>\n",
       "      <td>34</td>\n",
       "      <td>24.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>24.8</td>\n",
       "      <td>2024-03-29 22:00:00</td>\n",
       "      <td>00a719bb-2814-4ecf-814d-7ac4f862e834</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>13</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187366</th>\n",
       "      <td>26.1</td>\n",
       "      <td>34</td>\n",
       "      <td>24.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>24.8</td>\n",
       "      <td>2024-03-29 22:15:00</td>\n",
       "      <td>00a719bb-2814-4ecf-814d-7ac4f862e834</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>13</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187367</th>\n",
       "      <td>26.1</td>\n",
       "      <td>34</td>\n",
       "      <td>24.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>24.8</td>\n",
       "      <td>2024-03-29 22:30:00</td>\n",
       "      <td>00a719bb-2814-4ecf-814d-7ac4f862e834</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>13</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187368</th>\n",
       "      <td>26.1</td>\n",
       "      <td>34</td>\n",
       "      <td>24.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>24.8</td>\n",
       "      <td>2024-03-29 22:45:00</td>\n",
       "      <td>00a719bb-2814-4ecf-814d-7ac4f862e834</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>13</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187369</th>\n",
       "      <td>25.2</td>\n",
       "      <td>35</td>\n",
       "      <td>23.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.3</td>\n",
       "      <td>21.2</td>\n",
       "      <td>2024-03-29 23:00:00</td>\n",
       "      <td>00a719bb-2814-4ecf-814d-7ac4f862e834</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>13</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>187370 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        temperature_2m  relative_humidity_2m  apparent_temperature  \\\n",
       "0                 30.7                    33                  31.7   \n",
       "1                 30.7                    33                  31.7   \n",
       "2                 30.7                    33                  31.7   \n",
       "3                 30.7                    33                  31.7   \n",
       "4                 31.1                    34                  32.0   \n",
       "...                ...                   ...                   ...   \n",
       "187365            26.1                    34                  24.3   \n",
       "187366            26.1                    34                  24.3   \n",
       "187367            26.1                    34                  24.3   \n",
       "187368            26.1                    34                  24.3   \n",
       "187369            25.2                    35                  23.5   \n",
       "\n",
       "        precipitation  wind_speed_10m  wind_speed_100m       creation_time  \\\n",
       "0                 0.0            11.0             13.6 2024-03-22 08:00:00   \n",
       "1                 0.0            11.0             13.6 2024-03-22 08:15:00   \n",
       "2                 0.0            11.0             13.6 2024-03-22 08:30:00   \n",
       "3                 0.0            11.0             13.6 2024-03-22 08:45:00   \n",
       "4                 0.0            11.2             13.8 2024-03-22 09:00:00   \n",
       "...               ...             ...              ...                 ...   \n",
       "187365            0.0            10.9             24.8 2024-03-29 22:00:00   \n",
       "187366            0.0            10.9             24.8 2024-03-29 22:15:00   \n",
       "187367            0.0            10.9             24.8 2024-03-29 22:30:00   \n",
       "187368            0.0            10.9             24.8 2024-03-29 22:45:00   \n",
       "187369            0.0             9.3             21.2 2024-03-29 23:00:00   \n",
       "\n",
       "                                   sensor_id  count  is_holiday  ...  lag6  \\\n",
       "0       022040a0-beb4-413c-bc3b-e6564fc2699c      9           0  ...   NaN   \n",
       "1       022040a0-beb4-413c-bc3b-e6564fc2699c     10           0  ...   NaN   \n",
       "2       022040a0-beb4-413c-bc3b-e6564fc2699c      9           0  ...   NaN   \n",
       "3       022040a0-beb4-413c-bc3b-e6564fc2699c      9           0  ...   NaN   \n",
       "4       022040a0-beb4-413c-bc3b-e6564fc2699c      9           0  ...   NaN   \n",
       "...                                      ...    ...         ...  ...   ...   \n",
       "187365  00a719bb-2814-4ecf-814d-7ac4f862e834      9           1  ...   1.0   \n",
       "187366  00a719bb-2814-4ecf-814d-7ac4f862e834      9           1  ...   0.5   \n",
       "187367  00a719bb-2814-4ecf-814d-7ac4f862e834      9           1  ...   0.5   \n",
       "187368  00a719bb-2814-4ecf-814d-7ac4f862e834      9           1  ...   0.5   \n",
       "187369  00a719bb-2814-4ecf-814d-7ac4f862e834      9           1  ...   0.5   \n",
       "\n",
       "        lag7  day  hour  month  dayofweek  quarter  dayofyear  weekofyear  \\\n",
       "0        NaN   22     8      3          4        1         82          12   \n",
       "1        NaN   22     8      3          4        1         82          12   \n",
       "2        NaN   22     8      3          4        1         82          12   \n",
       "3        NaN   22     8      3          4        1         82          12   \n",
       "4        NaN   22     9      3          4        1         82          12   \n",
       "...      ...  ...   ...    ...        ...      ...        ...         ...   \n",
       "187365   NaN   29    22      3          4        1         89          13   \n",
       "187366   NaN   29    22      3          4        1         89          13   \n",
       "187367   NaN   29    22      3          4        1         89          13   \n",
       "187368   NaN   29    22      3          4        1         89          13   \n",
       "187369   NaN   29    23      3          4        1         89          13   \n",
       "\n",
       "        year  \n",
       "0       2024  \n",
       "1       2024  \n",
       "2       2024  \n",
       "3       2024  \n",
       "4       2024  \n",
       "...      ...  \n",
       "187365  2024  \n",
       "187366  2024  \n",
       "187367  2024  \n",
       "187368  2024  \n",
       "187369  2024  \n",
       "\n",
       "[187370 rows x 26 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf = pd.DataFrame(data)\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temperature_2m</th>\n",
       "      <th>relative_humidity_2m</th>\n",
       "      <th>apparent_temperature</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>wind_speed_10m</th>\n",
       "      <th>wind_speed_100m</th>\n",
       "      <th>creation_time</th>\n",
       "      <th>sensor_id</th>\n",
       "      <th>count</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>...</th>\n",
       "      <th>lag6</th>\n",
       "      <th>lag7</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>month</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>quarter</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30.7</td>\n",
       "      <td>33</td>\n",
       "      <td>31.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.6</td>\n",
       "      <td>2024-03-22 08:00:00</td>\n",
       "      <td>022040a0-beb4-413c-bc3b-e6564fc2699c</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>12</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.7</td>\n",
       "      <td>33</td>\n",
       "      <td>31.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.6</td>\n",
       "      <td>2024-03-22 08:15:00</td>\n",
       "      <td>022040a0-beb4-413c-bc3b-e6564fc2699c</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>12</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30.7</td>\n",
       "      <td>33</td>\n",
       "      <td>31.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.6</td>\n",
       "      <td>2024-03-22 08:30:00</td>\n",
       "      <td>022040a0-beb4-413c-bc3b-e6564fc2699c</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>12</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.7</td>\n",
       "      <td>33</td>\n",
       "      <td>31.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.6</td>\n",
       "      <td>2024-03-22 08:45:00</td>\n",
       "      <td>022040a0-beb4-413c-bc3b-e6564fc2699c</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>12</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31.1</td>\n",
       "      <td>34</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.2</td>\n",
       "      <td>13.8</td>\n",
       "      <td>2024-03-22 09:00:00</td>\n",
       "      <td>022040a0-beb4-413c-bc3b-e6564fc2699c</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>12</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187365</th>\n",
       "      <td>26.1</td>\n",
       "      <td>34</td>\n",
       "      <td>24.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>24.8</td>\n",
       "      <td>2024-03-29 22:00:00</td>\n",
       "      <td>00a719bb-2814-4ecf-814d-7ac4f862e834</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>13</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187366</th>\n",
       "      <td>26.1</td>\n",
       "      <td>34</td>\n",
       "      <td>24.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>24.8</td>\n",
       "      <td>2024-03-29 22:15:00</td>\n",
       "      <td>00a719bb-2814-4ecf-814d-7ac4f862e834</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>13</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187367</th>\n",
       "      <td>26.1</td>\n",
       "      <td>34</td>\n",
       "      <td>24.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>24.8</td>\n",
       "      <td>2024-03-29 22:30:00</td>\n",
       "      <td>00a719bb-2814-4ecf-814d-7ac4f862e834</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>13</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187368</th>\n",
       "      <td>26.1</td>\n",
       "      <td>34</td>\n",
       "      <td>24.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>24.8</td>\n",
       "      <td>2024-03-29 22:45:00</td>\n",
       "      <td>00a719bb-2814-4ecf-814d-7ac4f862e834</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>13</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187369</th>\n",
       "      <td>25.2</td>\n",
       "      <td>35</td>\n",
       "      <td>23.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.3</td>\n",
       "      <td>21.2</td>\n",
       "      <td>2024-03-29 23:00:00</td>\n",
       "      <td>00a719bb-2814-4ecf-814d-7ac4f862e834</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>13</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>187370 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        temperature_2m  relative_humidity_2m  apparent_temperature  \\\n",
       "0                 30.7                    33                  31.7   \n",
       "1                 30.7                    33                  31.7   \n",
       "2                 30.7                    33                  31.7   \n",
       "3                 30.7                    33                  31.7   \n",
       "4                 31.1                    34                  32.0   \n",
       "...                ...                   ...                   ...   \n",
       "187365            26.1                    34                  24.3   \n",
       "187366            26.1                    34                  24.3   \n",
       "187367            26.1                    34                  24.3   \n",
       "187368            26.1                    34                  24.3   \n",
       "187369            25.2                    35                  23.5   \n",
       "\n",
       "        precipitation  wind_speed_10m  wind_speed_100m       creation_time  \\\n",
       "0                 0.0            11.0             13.6 2024-03-22 08:00:00   \n",
       "1                 0.0            11.0             13.6 2024-03-22 08:15:00   \n",
       "2                 0.0            11.0             13.6 2024-03-22 08:30:00   \n",
       "3                 0.0            11.0             13.6 2024-03-22 08:45:00   \n",
       "4                 0.0            11.2             13.8 2024-03-22 09:00:00   \n",
       "...               ...             ...              ...                 ...   \n",
       "187365            0.0            10.9             24.8 2024-03-29 22:00:00   \n",
       "187366            0.0            10.9             24.8 2024-03-29 22:15:00   \n",
       "187367            0.0            10.9             24.8 2024-03-29 22:30:00   \n",
       "187368            0.0            10.9             24.8 2024-03-29 22:45:00   \n",
       "187369            0.0             9.3             21.2 2024-03-29 23:00:00   \n",
       "\n",
       "                                   sensor_id  count  is_holiday  ...  lag6  \\\n",
       "0       022040a0-beb4-413c-bc3b-e6564fc2699c      9           0  ...   NaN   \n",
       "1       022040a0-beb4-413c-bc3b-e6564fc2699c     10           0  ...   NaN   \n",
       "2       022040a0-beb4-413c-bc3b-e6564fc2699c      9           0  ...   NaN   \n",
       "3       022040a0-beb4-413c-bc3b-e6564fc2699c      9           0  ...   NaN   \n",
       "4       022040a0-beb4-413c-bc3b-e6564fc2699c      9           0  ...   NaN   \n",
       "...                                      ...    ...         ...  ...   ...   \n",
       "187365  00a719bb-2814-4ecf-814d-7ac4f862e834      9           1  ...   1.0   \n",
       "187366  00a719bb-2814-4ecf-814d-7ac4f862e834      9           1  ...   0.5   \n",
       "187367  00a719bb-2814-4ecf-814d-7ac4f862e834      9           1  ...   0.5   \n",
       "187368  00a719bb-2814-4ecf-814d-7ac4f862e834      9           1  ...   0.5   \n",
       "187369  00a719bb-2814-4ecf-814d-7ac4f862e834      9           1  ...   0.5   \n",
       "\n",
       "        lag7  day  hour  month  dayofweek  quarter  dayofyear  weekofyear  \\\n",
       "0        NaN   22     8      3          4        1         82          12   \n",
       "1        NaN   22     8      3          4        1         82          12   \n",
       "2        NaN   22     8      3          4        1         82          12   \n",
       "3        NaN   22     8      3          4        1         82          12   \n",
       "4        NaN   22     9      3          4        1         82          12   \n",
       "...      ...  ...   ...    ...        ...      ...        ...         ...   \n",
       "187365   NaN   29    22      3          4        1         89          13   \n",
       "187366   NaN   29    22      3          4        1         89          13   \n",
       "187367   NaN   29    22      3          4        1         89          13   \n",
       "187368   NaN   29    22      3          4        1         89          13   \n",
       "187369   NaN   29    23      3          4        1         89          13   \n",
       "\n",
       "        year  \n",
       "0       2024  \n",
       "1       2024  \n",
       "2       2024  \n",
       "3       2024  \n",
       "4       2024  \n",
       "...      ...  \n",
       "187365  2024  \n",
       "187366  2024  \n",
       "187367  2024  \n",
       "187368  2024  \n",
       "187369  2024  \n",
       "\n",
       "[187370 rows x 26 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([59, 59, 59, ..., 19, 19, 19])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a LabelEncoder instance\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the LabelEncoder to the 'sensor_id' column and transform it\n",
    "pdf['encoded_sensor_id'] = label_encoder.fit_transform(pdf['sensor_id'])\n",
    "\n",
    "# Display the encoded DataFrame\n",
    "encoded_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temperature_2m</th>\n",
       "      <th>relative_humidity_2m</th>\n",
       "      <th>apparent_temperature</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>wind_speed_10m</th>\n",
       "      <th>wind_speed_100m</th>\n",
       "      <th>creation_time</th>\n",
       "      <th>sensor_id</th>\n",
       "      <th>count</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>...</th>\n",
       "      <th>lag7</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>month</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>quarter</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>year</th>\n",
       "      <th>encoded_sensor_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30.7</td>\n",
       "      <td>33</td>\n",
       "      <td>31.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.6</td>\n",
       "      <td>2024-03-22 08:00:00</td>\n",
       "      <td>022040a0-beb4-413c-bc3b-e6564fc2699c</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>12</td>\n",
       "      <td>2024</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.7</td>\n",
       "      <td>33</td>\n",
       "      <td>31.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.6</td>\n",
       "      <td>2024-03-22 08:15:00</td>\n",
       "      <td>022040a0-beb4-413c-bc3b-e6564fc2699c</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>12</td>\n",
       "      <td>2024</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30.7</td>\n",
       "      <td>33</td>\n",
       "      <td>31.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.6</td>\n",
       "      <td>2024-03-22 08:30:00</td>\n",
       "      <td>022040a0-beb4-413c-bc3b-e6564fc2699c</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>12</td>\n",
       "      <td>2024</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.7</td>\n",
       "      <td>33</td>\n",
       "      <td>31.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.6</td>\n",
       "      <td>2024-03-22 08:45:00</td>\n",
       "      <td>022040a0-beb4-413c-bc3b-e6564fc2699c</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>12</td>\n",
       "      <td>2024</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31.1</td>\n",
       "      <td>34</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.2</td>\n",
       "      <td>13.8</td>\n",
       "      <td>2024-03-22 09:00:00</td>\n",
       "      <td>022040a0-beb4-413c-bc3b-e6564fc2699c</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>12</td>\n",
       "      <td>2024</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187365</th>\n",
       "      <td>26.1</td>\n",
       "      <td>34</td>\n",
       "      <td>24.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>24.8</td>\n",
       "      <td>2024-03-29 22:00:00</td>\n",
       "      <td>00a719bb-2814-4ecf-814d-7ac4f862e834</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>13</td>\n",
       "      <td>2024</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187366</th>\n",
       "      <td>26.1</td>\n",
       "      <td>34</td>\n",
       "      <td>24.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>24.8</td>\n",
       "      <td>2024-03-29 22:15:00</td>\n",
       "      <td>00a719bb-2814-4ecf-814d-7ac4f862e834</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>13</td>\n",
       "      <td>2024</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187367</th>\n",
       "      <td>26.1</td>\n",
       "      <td>34</td>\n",
       "      <td>24.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>24.8</td>\n",
       "      <td>2024-03-29 22:30:00</td>\n",
       "      <td>00a719bb-2814-4ecf-814d-7ac4f862e834</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>13</td>\n",
       "      <td>2024</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187368</th>\n",
       "      <td>26.1</td>\n",
       "      <td>34</td>\n",
       "      <td>24.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>24.8</td>\n",
       "      <td>2024-03-29 22:45:00</td>\n",
       "      <td>00a719bb-2814-4ecf-814d-7ac4f862e834</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>13</td>\n",
       "      <td>2024</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187369</th>\n",
       "      <td>25.2</td>\n",
       "      <td>35</td>\n",
       "      <td>23.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.3</td>\n",
       "      <td>21.2</td>\n",
       "      <td>2024-03-29 23:00:00</td>\n",
       "      <td>00a719bb-2814-4ecf-814d-7ac4f862e834</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>13</td>\n",
       "      <td>2024</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>187370 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        temperature_2m  relative_humidity_2m  apparent_temperature  \\\n",
       "0                 30.7                    33                  31.7   \n",
       "1                 30.7                    33                  31.7   \n",
       "2                 30.7                    33                  31.7   \n",
       "3                 30.7                    33                  31.7   \n",
       "4                 31.1                    34                  32.0   \n",
       "...                ...                   ...                   ...   \n",
       "187365            26.1                    34                  24.3   \n",
       "187366            26.1                    34                  24.3   \n",
       "187367            26.1                    34                  24.3   \n",
       "187368            26.1                    34                  24.3   \n",
       "187369            25.2                    35                  23.5   \n",
       "\n",
       "        precipitation  wind_speed_10m  wind_speed_100m       creation_time  \\\n",
       "0                 0.0            11.0             13.6 2024-03-22 08:00:00   \n",
       "1                 0.0            11.0             13.6 2024-03-22 08:15:00   \n",
       "2                 0.0            11.0             13.6 2024-03-22 08:30:00   \n",
       "3                 0.0            11.0             13.6 2024-03-22 08:45:00   \n",
       "4                 0.0            11.2             13.8 2024-03-22 09:00:00   \n",
       "...               ...             ...              ...                 ...   \n",
       "187365            0.0            10.9             24.8 2024-03-29 22:00:00   \n",
       "187366            0.0            10.9             24.8 2024-03-29 22:15:00   \n",
       "187367            0.0            10.9             24.8 2024-03-29 22:30:00   \n",
       "187368            0.0            10.9             24.8 2024-03-29 22:45:00   \n",
       "187369            0.0             9.3             21.2 2024-03-29 23:00:00   \n",
       "\n",
       "                                   sensor_id  count  is_holiday  ...  lag7  \\\n",
       "0       022040a0-beb4-413c-bc3b-e6564fc2699c      9           0  ...   NaN   \n",
       "1       022040a0-beb4-413c-bc3b-e6564fc2699c     10           0  ...   NaN   \n",
       "2       022040a0-beb4-413c-bc3b-e6564fc2699c      9           0  ...   NaN   \n",
       "3       022040a0-beb4-413c-bc3b-e6564fc2699c      9           0  ...   NaN   \n",
       "4       022040a0-beb4-413c-bc3b-e6564fc2699c      9           0  ...   NaN   \n",
       "...                                      ...    ...         ...  ...   ...   \n",
       "187365  00a719bb-2814-4ecf-814d-7ac4f862e834      9           1  ...   NaN   \n",
       "187366  00a719bb-2814-4ecf-814d-7ac4f862e834      9           1  ...   NaN   \n",
       "187367  00a719bb-2814-4ecf-814d-7ac4f862e834      9           1  ...   NaN   \n",
       "187368  00a719bb-2814-4ecf-814d-7ac4f862e834      9           1  ...   NaN   \n",
       "187369  00a719bb-2814-4ecf-814d-7ac4f862e834      9           1  ...   NaN   \n",
       "\n",
       "        day  hour  month  dayofweek  quarter  dayofyear  weekofyear  year  \\\n",
       "0        22     8      3          4        1         82          12  2024   \n",
       "1        22     8      3          4        1         82          12  2024   \n",
       "2        22     8      3          4        1         82          12  2024   \n",
       "3        22     8      3          4        1         82          12  2024   \n",
       "4        22     9      3          4        1         82          12  2024   \n",
       "...     ...   ...    ...        ...      ...        ...         ...   ...   \n",
       "187365   29    22      3          4        1         89          13  2024   \n",
       "187366   29    22      3          4        1         89          13  2024   \n",
       "187367   29    22      3          4        1         89          13  2024   \n",
       "187368   29    22      3          4        1         89          13  2024   \n",
       "187369   29    23      3          4        1         89          13  2024   \n",
       "\n",
       "        encoded_sensor_id  \n",
       "0                      59  \n",
       "1                      59  \n",
       "2                      59  \n",
       "3                      59  \n",
       "4                      59  \n",
       "...                   ...  \n",
       "187365                 19  \n",
       "187366                 19  \n",
       "187367                 19  \n",
       "187368                 19  \n",
       "187369                 19  \n",
       "\n",
       "[187370 rows x 27 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = pdf.groupby(['encoded_sensor_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1230\n",
      "1\n",
      "2044\n",
      "2\n",
      "3080\n",
      "3\n",
      "2372\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "model_dict = {}\n",
    "for id , data in grouped_df:\n",
    "    print(id[0])\n",
    "    df = data.copy()\n",
    "    model = split_train_test(pdf=df)\n",
    "    print(len(df))\n",
    "    model_dict[id[0]] = model\n",
    "    i+=1\n",
    "    if i>3:\n",
    "        break\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...),\n",
       " 1: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...),\n",
       " 2: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...),\n",
       " 3: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(pdf):\n",
    "    total_rows = len(pdf)\n",
    "    train_rows = int(0.8 * total_rows)\n",
    "    pdf = pdf.sort_values(by=\"creation_time\")\n",
    "    train_df = pdf.iloc[:train_rows]\n",
    "    FEATURES = ['is_holiday','temperature_2m', 'relative_humidity_2m', 'apparent_temperature','precipitation', 'wind_speed_10m','wind_speed_100m', 'lag1', 'lag2','lag3', 'lag4', 'lag5', 'day', 'hour', 'month', 'dayofweek', 'quarter','dayofyear', \"encoded_sensor_id\",'weekofyear', 'year']\n",
    "    TARGET = ['consumed_unit']\n",
    "    \n",
    "    X_train = train_df[FEATURES]\n",
    "    y_train = train_df[TARGET]\n",
    "\n",
    "    # from xgboost import XGBRegressor\n",
    "    xgb_model = XGBRegressor()\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    param_grid = {\n",
    "    'n_estimators': [50, 100, 150, 200, 500],\n",
    "    'max_depth': [5, 10, 15, 20, 30],\n",
    "    'learning_rate': [0.01, 0.1, 0.3, 0.5, 0.7],\n",
    "    'reg_alpha':  [0.001, 0.01, 0.1, 0.5, 1.0]\n",
    "    }\n",
    "\n",
    "    random_search = RandomizedSearchCV(xgb_model,\n",
    "                                        param_distributions=param_grid,\n",
    "                                        n_iter=10,\n",
    "                                        scoring='neg_mean_squared_error',\n",
    "                                        cv=5,\n",
    "                                        # verbose=1,\n",
    "                                        n_jobs=-1,\n",
    "                                        random_state=45)\n",
    "\n",
    "    # Fit the RandomizedSearchCV to the data\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best parameters\n",
    "    best_params = random_search.best_params_\n",
    "    # print(f\"Best Parameters for sensor {i}: {best_params}\")\n",
    "\n",
    "    # Train the model with the best parameters\n",
    "    best_xgb_model = XGBRegressor(n_estimators=best_params['n_estimators'],\n",
    "                                max_depth=best_params['max_depth'],\n",
    "                                learning_rate=best_params['learning_rate'],\n",
    "                                reg_alpha=best_params['reg_alpha'],\n",
    "                                reg_lambda=0.01,\n",
    "                                )\n",
    "    model = best_xgb_model.fit(X_train, y_train)\n",
    "    return model\n",
    "# result = (store_part.groupBy(\"sensor_id\").apply(train_test_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "No evaluation result, `eval_set` is not used during training.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Deployment\\spark_job\\spark\\Lib\\site-packages\\xgboost\\sklearn.py:1260\u001b[0m, in \u001b[0;36mXGBModel.evals_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1258\u001b[0m     evals_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevals_result_\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1260\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(\n\u001b[0;32m   1261\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo evaluation result, `eval_set` is not used during training.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1262\u001b[0m     )\n\u001b[0;32m   1264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m evals_result\n",
      "\u001b[1;31mXGBoostError\u001b[0m: No evaluation result, `eval_set` is not used during training."
     ]
    }
   ],
   "source": [
    "model.evals_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.joblib']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_filename = f\"model.joblib\"\n",
    "joblib.dump(model_dict, model_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=500, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;XGBRegressor<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=500, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=500, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = joblib.load(\"model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-3 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-3 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-3 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-3 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-3 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=500, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;XGBRegressor<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=500, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=500, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml.get(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = df.rdd.getNumPartitions()\n",
    "print(\"number of partion before:\",num)\n",
    "# df_repartitoned = df.repartition(\"sensor_id\")\n",
    "# num2 = df_repartitoned.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store_part = spark.sql(sql).repartition(spark.sparkContext.defaultParrallelism,['sensor_id']).cache()\n",
    "store_part = spark.sql(sql).repartition(spark.sparkContext.defaultParallelism, ['sensor_id']).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_part.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType,IntegerType,StringType,TimestampType,StructType,StructField\n",
    "\n",
    "result_schema = StructType([\n",
    "    StructField(\"temperature_2m\", FloatType(), True),\n",
    "    StructField(\"relative_humidity_2m\", IntegerType(), True),\n",
    "    StructField(\"apparent_temperature\", FloatType(), True),\n",
    "    StructField(\"precipitation\", FloatType(), True),\n",
    "    StructField(\"wind_speed_10m\", FloatType(), True),\n",
    "    StructField(\"wind_speed_100m\", FloatType(), True),\n",
    "    StructField(\"creation_time\", TimestampType(), True),\n",
    "    StructField(\"sensor_id\", StringType(), True),\n",
    "    StructField(\"count\", IntegerType(), True),\n",
    "    StructField(\"is_holiday\", IntegerType(), True),\n",
    "    StructField(\"consumed_unit\", FloatType(), True),\n",
    "    StructField(\"lag1\", FloatType(), True),\n",
    "    StructField(\"lag2\", FloatType(), True),\n",
    "    StructField(\"lag3\", FloatType(), True),\n",
    "    StructField(\"lag4\", FloatType(), True),\n",
    "    StructField(\"lag5\", FloatType(), True),\n",
    "    StructField(\"lag6\", FloatType(), True),\n",
    "    StructField(\"lag7\", FloatType(), True),\n",
    "    StructField(\"day\", IntegerType(), True),\n",
    "    StructField(\"hour\", IntegerType(), True),\n",
    "    StructField(\"month\", IntegerType(), True),\n",
    "    StructField(\"dayofweek\", IntegerType(), True),\n",
    "    StructField(\"quarter\", IntegerType(), True),\n",
    "    StructField(\"dayofyear\", IntegerType(), True),\n",
    "    StructField(\"weekofyear\", IntegerType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "# @pandas_udf(result_schema,PandasUDFType.GROUPED_MAP)\n",
    "@pandas_udf(result_schema, PandasUDFType.GROUPED_MAP)\n",
    "# def load_forecast(senor_data):\n",
    "def train_test_split(df):\n",
    "    try:\n",
    "        print(\"hello\")\n",
    "        # total_rows = df.count()\n",
    "\n",
    "        # test_rows = int(0.2 * total_rows)\n",
    "        # train_rows = max(0, total_rows - test_rows)\n",
    "        # test = df.orderBy(col(\"creation_time\").desc()).limit(test_rows)\n",
    "        # train = df.orderBy(col(\"creation_time\").asc()).limit(train_rows)\n",
    "        # return [(train,test)]\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(\"error in spliting data:\",e)\n",
    "# result = (store_part.groupBy(\"sensor_id\").apply(train_test_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "# import pandas as pd\n",
    "# @pandas_udf(result_schema,PandasUDFType.GROUPED_MAP)\n",
    "# def train_test_split(df):\n",
    "#     try:\n",
    "#         print(\"hello\")\n",
    "#         return pd.DataFrame({\"result\": [df]})  # Example return, adjust as needed\n",
    "#     except Exception as e:\n",
    "#         print(\"error in splitting data:\", e)\n",
    "\n",
    "result = (store_part.groupBy(\"sensor_id\").apply(train_test_split))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "names = [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\", \"Frank\", \"Grace\", \"Henry\", \"Ivy\", \"Jack\"]\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Department\", StringType(), True)\n",
    "])\n",
    "# List of departments\n",
    "departments = [\"HR\", \"IT\", \"Finance\", \"Marketing\", \"Operations\"]\n",
    "\n",
    "# Randomly choose departments for each person\n",
    "data = [(name, random.choice(departments)) for name in names]\n",
    "\n",
    "# Extend the data to have some repetitive departments\n",
    "data_extended = data + data[:3]  # Repeat the first three departments\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data_extended, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repartitoned = df.repartition(\"department\")\n",
    "print(df_repartitoned.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def f(people):\n",
    "#     for person in people:\n",
    "#         print(person.Name)\n",
    "# df_repartitoned.foreachPartition(f)\n",
    "\n",
    "def f(iterator):\n",
    "    for person in iterator:\n",
    "        print(person.Name)\n",
    "\n",
    "# Apply the function to each partition\n",
    "df_repartitoned.foreachPartition(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(iterator):\n",
    "    names = [person.Name for person in iterator]\n",
    "    print(names)\n",
    "\n",
    "# Apply the function to each partition and collect the results to the driver\n",
    "df_repartitoned.foreachPartition(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\", \"Frank\", \"Grace\", \"Henry\", \"Ivy\", \"Jack\"]\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Department\", StringType(), True)\n",
    "])\n",
    "departments = [\"HR\", \"IT\", \"Finance\", \"Marketing\", \"Operations\"]\n",
    "data = [(name, random.choice(departments)) for name in names]\n",
    "df = spark.createDataFrame(data_extended, schema)\n",
    "# df.show()\n",
    "\n",
    "# Repartition the DataFrame based on the \"Department\" column\n",
    "df_repartitioned = df.repartition(\"Department\")\n",
    "print(df_repartitioned.rdd.getNumPartitions())\n",
    "# Print names within each partition using collect() and a loop\n",
    "# if df_repartitioned.rdd.isEmpty():\n",
    "#     print(\"DataFrame is empty\")\n",
    "# else:\n",
    "#     # Get names from each partition using collect()\n",
    "#     partitions = df_repartitioned.rdd.collect()\n",
    "\n",
    "#     # Loop through partitions and print names\n",
    "#     for partition in partitions:\n",
    "#         partition.show()\n",
    "#         names = [person.Name for person in partition]\n",
    "#         print(f\"Names in partition: {names}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repartitioned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_consumption_df = df.groupBy(\"sensor_id\").agg(sum(\"consumed_unit\").alias(\"total_consumption\"))\n",
    "total_consumption_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repartitioned = df.repartition(\"sensor_id\")\n",
    "# Define a UDF to encapsulate your calculations\n",
    "def analyze_timeseries(data):\n",
    "  # Your time series analysis logic goes here\n",
    "  # Example: Calculate mean consumption\n",
    "  mean_consumption = data.select(\"consumed_unit\").mean()  # Access the DataFrame within the UDF\n",
    "  return mean_consumption\n",
    "\n",
    "analyze_timeseries_udf = spark.udf.register(\"analyze_timeseries\", analyze_timeseries)\n",
    "\n",
    "# Apply the UDF to each partition (sensor)\n",
    "analyzed_df = df_repartitioned.withColumn(\"analysis_result\", analyze_timeseries_udf(col(\"consumed_unit\")))\n",
    "analyzed_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "partitioned_df = df.repartition(\"sensor_id\") \n",
    "\n",
    "# Define a UDF to encapsulate calculations (using PySpark functions for clarity)\n",
    "def analyze_timeseries(data):\n",
    "  print(data)\n",
    "  mean_consumption = F.mean(data[\"consumed_unit\"])\n",
    "  return mean_consumption\n",
    "\n",
    "analyze_timeseries_udf = spark.udf.register(\"analyze_timeseries\", analyze_timeseries)\n",
    "\n",
    "# Apply the UDF to each partition (sensor), ensuring type safety\n",
    "analyzed_df = partitioned_df.withColumn(\"analysis_result\", analyze_timeseries_udf(col(\"consumed_unit\").cast(\"double\")))\n",
    "\n",
    "# View the results\n",
    "analyzed_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped_data = df.groupBy(\"sensor_id\")\n",
    "df_repartitoned = df.repartition\n",
    "\n",
    "def train_test_split(df):\n",
    "    try:\n",
    "\n",
    "        total_rows = df.count()\n",
    "        test_rows = int(0.2 * total_rows)\n",
    "        train_rows = max(0, total_rows - test_rows)\n",
    "        print(total_rows,test_rows,train_rows)\n",
    "        test = df.orderBy(col(\"creation_time\").desc()).limit(test_rows)\n",
    "        train = df.orderBy(col(\"creation_time\").asc()).limit(train_rows)\n",
    "        return train,test\n",
    "    except Exception as e:\n",
    "        print(\"error in spliting data:\",e)\n",
    "train_test_splits = grouped_data.flatMap(lambda group: train_test_split(group[1]))\n",
    "\n",
    "train_test_splits = grouped_data.apply(train_test_split)\n",
    "\n",
    "# Collect train-test splits\n",
    "train_test_splits = train_test_splits.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"value\", StringType(), True),\n",
    "    StructField(\"creation_time\", StringType(), True),\n",
    "    StructField(\"sensor_id\", StringType(), True)])\n",
    "\n",
    "def train_test_split(key, iterator):\n",
    "    df = pd.DataFrame(list(iterator), columns=df.columns)\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    yield train_df, test_df\n",
    "\n",
    "# Apply train-test split function to each group in parallel\n",
    "train_test_splits = df.groupBy(\"sensor_id\").applyInPandas(train_test_split, schema)\n",
    "\n",
    "# Collect train-test splits\n",
    "train_test_splits_list = train_test_splits.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first few rows of each train-test split DataFrame\n",
    "for train_test_split_df in train_test_splits.take(5):\n",
    "    train_test_split_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_file_path = \"schema.yaml\"\n",
    "# Validate the data against the schema\n",
    "validated_df = reading_and_validate_data(spark, data, schema_file_path)\n",
    "\n",
    "# Check if validation was successful\n",
    "if validated_df is not None:\n",
    "    # validated_df.show()\n",
    "    df = validated_df.select(\"creation_time\",\"sensor_id\",\"consumed_unit\")\n",
    "    # df = validated_df\n",
    "    df.show()\n",
    "    df.createOrReplaceTempView(\"data\")\n",
    "    spark.sql(\"\"\"\n",
    "    select sensor_id,sum(consumed_unit) as units, count(sensor_id) as data_count\n",
    "    from data\n",
    "    group by sensor_id\n",
    "    order by data_count desc\n",
    "    \"\"\").show()\n",
    "else:\n",
    "    print(\"Data validation failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = sdf.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "test_size = 24 * 6 * 4  # 7 days * 24 hours/day\n",
    "gap = 24*4  # 1 day * 24 hours/day\n",
    "\n",
    "total_size = test_size + gap\n",
    "\n",
    "tss = TimeSeriesSplit(n_splits=3, test_size=total_size, gap=gap)\n",
    "\n",
    "for train_idx, val_idx in tss.split(pandas_df):\n",
    "    train_data = pandas_df.iloc[train_idx]\n",
    "    test_data = pandas_df.iloc[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 18,6\n",
    "plt.plot(test_data[\"creation_time\"],test_data['consumed_unit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = sdf.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.select(count(\"sensor_id\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.select(count(\"sensor_id\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_rows = 7 * 24 * 4\n",
    "\n",
    "def train_test_split(df):\n",
    "    try:\n",
    "        total_rows = df.count()\n",
    "        test_rows = int(0.2 * total_rows)\n",
    "        train_rows = max(0, total_rows - test_rows)\n",
    "        test = df.orderBy(col(\"creation_time\").desc()).limit(test_rows)\n",
    "        train = df.orderBy(col(\"creation_time\").asc()).limit(train_rows)\n",
    "        return train,test\n",
    "    except Exception as e:\n",
    "        print(\"error in spliting data:\",e)\n",
    "        \n",
    "def train_test_split_multi_timeseries(df):\n",
    "    window_spec = window.orderBy(col(\"creation_time\").asc())\n",
    "\n",
    "    # Calculate total and test rows (adjust test_size as needed)\n",
    "    total_rows = df.count()\n",
    "    test_rows = int(0.2 * total_rows)\n",
    "\n",
    "    # Ensure train_rows is not negative\n",
    "    train_rows = max(0, total_rows - test_rows)\n",
    "\n",
    "    # Apply window and filter for training and testing data\n",
    "    df_with_window = df.withColumn(\"window_id\", window_spec)    \n",
    "    train = df_with_window.filter(col(\"window_id\") <= train_rows)\n",
    "    test = df_with_window.filter(col(\"window_id\") > train_rows).orderBy(col(\"creation_time\").desc())\n",
    "    return train, test\n",
    "    window_spec = window.orderBy(col(\"creation_time\").asc())  # Order by ascending time\n",
    "    window_spec = window.orderBy(col(\"creation_time\").asc())  # Order by ascending creation time\n",
    "\n",
    "    # Calculate total and test rows (adjust test_size as needed)\n",
    "    total_rows = df.count()\n",
    "    test_rows = int(0.2 * total_rows)\n",
    "\n",
    "    # Ensure train_rows is not negative\n",
    "    train_rows = max(0, total_rows - test_rows)  # Handle potential negative train_rows\n",
    "\n",
    "    # Apply window and filter for training and testing data\n",
    "    df_with_window = df.withColumn(\"window_id\", window_spec)\n",
    "    train = df_with_window.filter(col(\"window_id\") <= train_rows)\n",
    "    test = df_with_window.filter(col(\"window_id\") > train_rows).orderBy(col(\"creation_time\").desc())  # Descending order for test\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define the split ratios (e.g., 80% training, 20% testing)\n",
    "train_ratio = 0.8\n",
    "test_ratio = 1 - train_ratio\n",
    "\n",
    "# Define a window partitioned by sensor_id and ordered by creation_time\n",
    "window_spec = Window.partitionBy(\"sensor_id\").orderBy(\"creation_time\")\n",
    "\n",
    "# Add row numbers within each partition (sensor_id)\n",
    "sdf_with_row_number = sdf.withColumn(\"row_number\", row_number().over(window_spec))\n",
    "\n",
    "# Calculate the split row numbers for each sensor group\n",
    "split_row_number = (col(\"max_row_number\") * train_ratio).cast(\"int\")\n",
    "\n",
    "# Split the data into training and testing sets based on row numbers\n",
    "train_data = sdf_with_row_number.filter(col(\"row_number\") <= split_row_number)\n",
    "test_data = sdf_with_row_number.filter(col(\"row_number\") > split_row_number)\n",
    "\n",
    "# Drop the row_number column from the final DataFrames\n",
    "train_data = train_data.drop(\"row_number\")\n",
    "test_data = test_data.drop(\"row_number\")\n",
    "\n",
    "# Show the number of rows in each set\n",
    "print(\"Number of rows in training set:\", train_data.count())\n",
    "print(\"Number of rows in testing set:\", test_data.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split_multi_timeseries(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.select(count(\"sensor_id\")).show()\n",
    "train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.select(count(\"sensor_id\")).show()\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window, functions as F\n",
    "\n",
    "\n",
    "def train_test_split(df):\n",
    "    \"\"\"Splits a DataFrame into train and test sets based on time series.\"\"\"\n",
    "\n",
    "    try:\n",
    "        window_spec = Window.orderBy(F.col(\"creation_time\").asc()) \\\n",
    "                           .partitionBy(\"sensor_id\") \\\n",
    "                           .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "        df = df.withColumn(\"row_number\", F.row_number().over(window_spec))\n",
    "\n",
    "        total_rows = df.count()\n",
    "        test_rows = int(0.2 * sdf.count())\n",
    "        train_rows = total_rows - test_rows\n",
    "\n",
    "        # Ensure train_rows is not negative\n",
    "        if train_rows < 0:\n",
    "            raise ValueError(\"Test rows exceed total rows. Adjust the test size.\")\n",
    "\n",
    "        test = df.orderBy(col(\"creation_time\").desc()).limit(test_rows)\n",
    "        train = df.orderBy(col(\"creation_time\").asc()).limit(train_rows)\n",
    "        total_rows = df.count()\n",
    "        train_threshold = total_rows - test_rows\n",
    "\n",
    "        train_df = df.where(F.col(\"row_number\") <= train_threshold)\n",
    "        test_df = df.filter(F.col(\"row_number\") > train_threshold)\n",
    "\n",
    "        return train_df, test_df\n",
    "    except Exception as e:\n",
    "        print(\"Error in splitting data:\", e)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_df, test_df = train_test_split(df.copy())  # Make a copy to avoid modifying original DataFrame\n",
    "\n",
    "# Now you can use train_df and test_df for further analysis or machine learning tasks\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(count_distinct(\"sensor_id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy(\"sensor_id\")\n",
    "# data = sdf.withColumn(\"total_units\"),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = sdf.withColumn(\"total_units\", sum(\"consumed_unit\").over(window_spec)) \\\n",
    "               .withColumn(\"average_units\", avg(\"consumed_unit\").over(window_spec))\n",
    "\n",
    "# Display the result\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf\n",
    "test_data_size = 24 * 4 * 7  # 24 hours * 4 quarters per hour * 7 days\n",
    "\n",
    "# Sort the DataFrame by creation_time\n",
    "sorted_df = df.sort(col(\"creation_time\"))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data = sorted_df.limit(sorted_df.count() - test_data_size)\n",
    "test_data = sorted_df.limit(test_data_size)\n",
    "\n",
    "# Show the schemas of the train and test DataFrames\n",
    "train_data.printSchema()\n",
    "test_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "def train_test_split_per_sensor(df, test_size_days=7, gap_days=1):\n",
    "    \"\"\"\n",
    "    Perform train-test split for each sensor_id using window functions.\n",
    "\n",
    "    Parameters:\n",
    "    - df: PySpark DataFrame containing the time series data with columns \"creation_time\" and \"sensor_id\".\n",
    "    - test_size_days: Size of the test set in days.\n",
    "    - gap_days: Gap between test sets in days.\n",
    "\n",
    "    Returns:\n",
    "    - train_data: PySpark DataFrame containing the training data.\n",
    "    - test_data: PySpark DataFrame containing the test data.\n",
    "    \"\"\"\n",
    "    # Define window specification partitioned by sensor_id and ordered by creation_time\n",
    "    window_spec = Window.partitionBy(\"sensor_id\").orderBy(\"creation_time\")\n",
    "\n",
    "    # Calculate test and train timestamps using lag function\n",
    "    df_with_timestamps = df.withColumn(\"test_end_timestamp\", F.lag(\"creation_time\", test_size_days).over(window_spec)) \\\n",
    "                           .withColumn(\"test_start_timestamp\", F.expr(f\"test_end_timestamp - INTERVAL {test_size_days} DAYS\")) \\\n",
    "                           .withColumn(\"train_end_timestamp\", F.expr(f\"test_start_timestamp - INTERVAL {gap_days} DAYS\")) \\\n",
    "                           .withColumn(\"train_start_timestamp\", F.first(\"creation_time\").over(window_spec))\n",
    "\n",
    "    # Perform train-test split based on calculated timestamps\n",
    "    train_data = df_with_timestamps.filter((F.col(\"creation_time\") >= F.col(\"train_start_timestamp\")) & (F.col(\"creation_time\") < F.col(\"train_end_timestamp\")))\n",
    "    test_data = df_with_timestamps.filter((F.col(\"creation_time\") >= F.col(\"test_start_timestamp\")) & (F.col(\"creation_time\") < F.col(\"test_end_timestamp\")))\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "# Example usage:\n",
    "train_data, test_data = train_test_split_per_sensor(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "test_size_intervals = 7 * 24 * 4  # 7 days * 24 hours/day * 4 intervals/hour (15 minutes)\n",
    "gap_intervals = 24 * 4  # 1 day * 24 hours/day * 4 intervals/hour (15 minutes)\n",
    "\n",
    "# Convert intervals to seconds\n",
    "test_size_seconds = test_size_intervals * 15 * 60\n",
    "gap_seconds = gap_intervals * 15 * 60\n",
    "\n",
    "# Function to perform train-test split for each sensor_id\n",
    "def split_train_test_per_sensor(rows):\n",
    "    for sensor_id, data_per_sensor in rows:\n",
    "        min_timestamp = data_per_sensor.select(F.min(\"creation_time\")).first()[0]\n",
    "        max_timestamp = data_per_sensor.select(F.max(\"creation_time\")).first()[0]\n",
    "        \n",
    "        # Calculate test start and end timestamps\n",
    "        test_end_timestamp = max_timestamp\n",
    "        test_start_timestamp = test_end_timestamp - test_size_seconds\n",
    "        \n",
    "        # Calculate train start and end timestamps\n",
    "        train_end_timestamp = test_start_timestamp - gap_seconds\n",
    "        train_start_timestamp = min_timestamp\n",
    "        \n",
    "        # Perform train-test split for the current sensor_id\n",
    "        train_data = data_per_sensor.filter((F.col(\"creation_time\") >= train_start_timestamp) & (F.col(\"creation_time\") < train_end_timestamp))\n",
    "        test_data = data_per_sensor.filter((F.col(\"creation_time\") >= test_start_timestamp) & (F.col(\"creation_time\") < test_end_timestamp))\n",
    "        \n",
    "        yield sensor_id, train_data, test_data\n",
    "\n",
    "# Perform groupBy operation on sensor_id and apply the split_train_test_per_sensor function\n",
    "train_test_per_sensor = df.groupBy(\"sensor_id\").mapPartitions(split_train_test_per_sensor)\n",
    "\n",
    "# Collect the results if needed\n",
    "results = train_test_per_sensor.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, col, lit\n",
    "\n",
    "def timeseries_train_test_split_sensor(df, training_ratio, test_size, gap_size):\n",
    "  \"\"\"\n",
    "  This function splits data for each unique sensor_id into training and testing sets \n",
    "  based on timestamps.\n",
    "\n",
    "  Args:\n",
    "      df: PySpark DataFrame containing time series data\n",
    "      training_ratio: Proportion of data allocated to training (0.0 to 1.0)\n",
    "      test_size: Number of time steps for the testing set\n",
    "      gap_size: Number of time steps between the training and testing sets\n",
    "\n",
    "  Returns:\n",
    "      A PySpark DataFrame containing sensor_id, training data, and testing data\n",
    "  \"\"\"\n",
    "\n",
    "  # Calculate training data size based on ratio\n",
    "  training_size = int(df.count() * training_ratio)\n",
    "\n",
    "  # Window function to calculate the training end timestamp\n",
    "  window_spec = window.orderBy(\"timestamp\").partitionBy(\"sensor_id\").rowsBetween(-test_size - gap_size, -1)\n",
    "  df_with_training_end_ts = df.withColumn(\"training_end_ts\", window_spec.end())\n",
    "\n",
    "  # Filter training and testing data based on timestamp\n",
    "  training_data = df.where(col(\"timestamp\") <= col(\"training_end_ts\"))\n",
    "  testing_data = df.where(col(\"timestamp\") > col(\"training_end_ts\"))\n",
    "\n",
    "  # Ensure training data covers training_size rows (optional)\n",
    "  training_data = training_data.limit(training_size)\n",
    "\n",
    "  # Combine sensor_id, training, and testing data into a single DataFrame\n",
    "  split_data = training_data.select(\"sensor_id\", \"timestamp\", col(\"*\").alias(\"training\"))\\\n",
    "               .union(testing_data.select(\"sensor_id\", \"timestamp\", col(\"*\").alias(\"testing\")))\n",
    "\n",
    "  return split_data\n",
    "\n",
    "# Split data for each sensor\n",
    "split_data = df.groupBy(\"sensor_id\").applyInPartitions(timeseries_train_test_split_sensor,lit(0.8), lit(24 * 7), lit(24))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, col, lit\n",
    "\n",
    "def timeseries_train_test_split_sensor(iterator, training_ratio, test_size, gap_size):\n",
    "    \"\"\"\n",
    "    This function splits data for each unique sensor_id into training and testing sets \n",
    "    based on timestamps (for mapPartitions).\n",
    "\n",
    "    Args:\n",
    "        iterator: Iterator over a partition of the grouped DataFrame\n",
    "        training_ratio: Proportion of data allocated to training (0.0 to 1.0)\n",
    "        test_size: Number of time steps for the testing set\n",
    "        gap_size: Number of time steps between the training and testing sets\n",
    "\n",
    "    Yields:\n",
    "        Rows containing sensor_id, training data, and testing data\n",
    "    \"\"\"\n",
    "    # Calculate training data size based on ratio\n",
    "    training_size = int(df.count() * training_ratio)\n",
    "\n",
    "    # Window function to calculate the training end timestamp\n",
    "    window_spec = window.orderBy(\"timestamp\").partitionBy(\"sensor_id\").rowsBetween(-test_size - gap_size, -1)\n",
    "    df_with_training_end_ts = df.withColumn(\"training_end_ts\", window_spec.end())\n",
    "\n",
    "    # Filter training and testing data based on timestamp\n",
    "    training_data = df.where(col(\"timestamp\") <= col(\"training_end_ts\"))\n",
    "    testing_data = df.where(col(\"timestamp\") > col(\"training_end_ts\"))\n",
    "\n",
    "    # Ensure training data covers training_size rows (optional)\n",
    "    training_data = training_data.limit(training_size)\n",
    "\n",
    "    # Combine sensor_id, training, and testing data into a single DataFrame\n",
    "    split_data = training_data.select(\"sensor_id\", \"timestamp\", col(\"*\").alias(\"training\"))\\\n",
    "                 .union(testing_data.select(\"sensor_id\", \"timestamp\", col(\"*\").alias(\"testing\")))\n",
    "\n",
    "    # Yield rows from the combined DataFrame\n",
    "    for row in split_data.rdd.collect():\n",
    "      yield row\n",
    "\n",
    "# Split data for each sensor\n",
    "split_data = df.groupBy(\"sensor_id\").mapPartitions(timeseries_train_test_split_sensor,lit(0.8), lit(24 * 7), lit(24))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col, lit, struct, window\n",
    "\n",
    "def train_test_split_udf(data, training_ratio, test_size, gap_size):\n",
    "  \"\"\"\n",
    "  This UDF splits data for a single sensor into training and testing sets \n",
    "  based on timestamps.\n",
    "\n",
    "  Args:\n",
    "      data: Sub-DataFrame for a single sensor_id\n",
    "      training_ratio: Proportion of data allocated to training (0.0 to 1.0)\n",
    "      test_size: Number of time steps for the testing set\n",
    "      gap_size: Number of time steps between the training and testing sets\n",
    "\n",
    "  Returns:\n",
    "      A tuple containing two DataFrames (training_data, testing_data)\n",
    "  \"\"\"\n",
    "\n",
    "  # Calculate training data size based on ratio\n",
    "  training_size = int(data.count() * training_ratio)\n",
    "\n",
    "  # Order data by timestamp\n",
    "  data = data.orderBy(\"timestamp\")  # Assuming you have a \"timestamp\" column\n",
    "\n",
    "  # Calculate end timestamp for training data (considering gap)\n",
    "  training_end_ts = data.select(window.last(\"timestamp\")).rdd.map(lambda r: r[0]).first() - \\\n",
    "                    lit(test_size + gap_size)\n",
    "\n",
    "  # Filter training and testing data based on timestamp\n",
    "  training_data = data.where(col(\"timestamp\") <= training_end_ts)\n",
    "  testing_data = data.where(col(\"timestamp\") > training_end_ts)\n",
    "\n",
    "  # Ensure training data covers training_size rows (optional)\n",
    "  training_data = training_data.limit(training_size)\n",
    "\n",
    "  return training_data, testing_data\n",
    "\n",
    "# Define UDF\n",
    "split_udf = udf(train_test_split_udf, structType=[training_data.schema, testing_data.schema])\n",
    "\n",
    "# Group by sensor_id and apply UDF\n",
    "split_data = df.groupBy(\"sensor_id\").apply(split_udf, lit(0.8), lit(24 * 7), lit(24))\n",
    "\n",
    "# Extract training and testing DataFrames from the split data\n",
    "split_data = split_data.select(\"sensor_id\", split_udf.alias(\"data\").explode())\n",
    "training_data = split_data.select(\"sensor_id\", \"data.training\")\n",
    "testing_data = split_data.select(\"sensor_id\", \"data.testing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_data(spark):\n",
    "    \"\"\"\n",
    "    processing data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        path1 = \"D:\\Projects\\Spark\\data.csv\"       \n",
    "        spark_df = spark.read.csv(path1,header= True)\n",
    "        spark_df.show()\n",
    "        spark_df.createOrReplaceTempView(\"data\")\n",
    "        spark.sql(\"\"\"\n",
    "        select count(*)\n",
    "        from data \n",
    "\n",
    "        \"\"\").show()\n",
    "    except Exception as e:\n",
    "        print(\"error in traininig:\",e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = initiate_spark()\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)  # Enable eager evaluation\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.maxNumRows\", 20)   # Set max number of rows to display\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.maxNumColumns\", 200)  # Set max number of columns to display\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 0)  # Disable truncation\n",
    "\n",
    "if spark:\n",
    "    try:\n",
    "        # training data\n",
    "        train_data(spark)\n",
    "\n",
    "        # display(df.select('sensor_id'))\n",
    "    finally:\n",
    "        # Terminate SparkSession\n",
    "        # terminate_spark(spark)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparkenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
